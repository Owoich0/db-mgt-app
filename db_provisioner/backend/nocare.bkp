import os
import shutil
import subprocess
import sqlite3
import requests
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from jinja2 import Environment, FileSystemLoader
from datetime import datetime
import re
import ast
import json
import yaml

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class DeployRequest(BaseModel):
    cluster_name: str
    platform: str
    instance_count: int
    postgresql_version: str
    ami: str = None
    instance_type: str = None
    data_volume_size: int = 0
    allowed_ip_1: str = None
    allowed_ip_2: str = None

class CreateDBRequest(BaseModel):
    cluster_name: str
    db_name: str

class DropDBRequest(BaseModel):
    cluster_name: str
    db_name: str

class StartRequest(BaseModel):
    cluster_name: str

class DecommissionRequest(BaseModel):
    cluster_name: str

class AddUserRequest(BaseModel):
    cluster_name: str
    username: str
    password: str
    database: str = "postgres"
    roles: list[str] = []

class RemoveUserRequest(BaseModel):
    cluster_name: str
    username: str

OS_AMI_USER_MAPPING = {
    "ami-0a73e96a849c232cc": "rocky",
    "ami-0c2b8ca1dad447f8a": "ubuntu",
    "ami-0de53d8956e8dcf80": "ec2-user",
    "ami-08962a4068733a2b6": "centos"
}

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMPLATE_DIR = os.path.join(BASE_DIR, "templates")
DEPLOYMENTS_DIR = os.path.join(BASE_DIR, "deployments")
db_path = os.path.join(BASE_DIR, "clusters.db")
env = Environment(loader=FileSystemLoader(TEMPLATE_DIR))

def init_db():
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS clusters (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            cluster_name TEXT,
            platform TEXT,
            status TEXT,
            instance_count INTEGER,
            ami TEXT,
            instance_type TEXT,
            data_volume_size INTEGER,
            postgresql_version TEXT,
            allowed_ip_1 TEXT,
            allowed_ip_2 TEXT,
            server_public_ip TEXT,
            deployment_dir TEXT,
            timestamp TEXT
        )
    ''')
    conn.commit()
    conn.close()

init_db()

@app.get("/os-options")
def get_os_options():
    return [{"ami": ami, "os": user} for ami, user in OS_AMI_USER_MAPPING.items()]

@app.get("/pg-versions")
def get_pg_versions():
    return ["14", "15", "16"]

@app.get("/clusters")
def list_clusters():
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT cluster_name, status, timestamp FROM clusters")
    rows = cursor.fetchall()
    conn.close()
    return [{"name": name, "status": status, "timestamp": ts} for name, status, ts in rows]

@app.get("/clusters/{name}/connection_info")
def get_connection_info(name: str):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT cluster_name, platform FROM clusters WHERE cluster_name=?", (name,))
    row = cursor.fetchone()
    conn.close()
    if not row:
        raise HTTPException(status_code=404, detail="Cluster not found")

    cluster_name, platform = row
    if platform == "kubernetes":
        return {
            "host": f"{name}.local.cluster",
            "port": 5432,
            "user": "postgres",
            "password": "postgres",
            "dbname": "postgres"
        }
    return {
        "host": f"{name}-node-1",
        "port": 5432,
        "user": "postgres",
        "password": "postgres",
        "dbname": "postgres"
    }

@app.post("/deploy")
def deploy_cluster(request: DeployRequest):
    if request.instance_count > 5:
        raise HTTPException(status_code=400, detail="Maximum 5 instances allowed.")

    cluster_dir = os.path.join(DEPLOYMENTS_DIR, request.cluster_name)
    if os.path.exists(cluster_dir):
        raise HTTPException(status_code=409, detail="Cluster already exists.")

    os.makedirs(cluster_dir, exist_ok=True)

    try:
        public_ip = requests.get("https://api.ipify.org").text.strip()
        ansible_dst = os.path.join(cluster_dir, "ansible")
        os.makedirs(ansible_dst, exist_ok=True)

        if request.platform == "kubernetes":
            shutil.copytree(os.path.join(TEMPLATE_DIR, "ansible"), ansible_dst, dirs_exist_ok=True)
            groupvars_path = os.path.join(ansible_dst, "group_vars", "all.yml")

            groupvars_template = env.get_template("ansible/group_vars/all.yml")
            with open(groupvars_path, "w") as f:
                f.write(groupvars_template.render(
                    cluster_name=request.cluster_name,
                    postgresql_version=request.postgresql_version,
                    instance_count=request.instance_count,
                    allowed_ips=[request.allowed_ip_1, request.allowed_ip_2],
                    server_public_ip=public_ip
                ))

            # Run Ansible playbook to install k3s and deploy PostgreSQL
            inventory_file = os.path.join(ansible_dst, "inventory", "inventory.ini")
            playbook_file = os.path.join(ansible_dst, "site.yml")

            subprocess.run(["ansible-playbook", "-i", inventory_file, playbook_file], check=True)

            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO clusters (
                    cluster_name, platform, instance_count, status, timestamp,
                    deployment_dir, ami, instance_type, data_volume_size,
                    postgresql_version, allowed_ip_1, allowed_ip_2, server_public_ip
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                request.cluster_name, request.platform, request.instance_count,
                "completed", datetime.utcnow().isoformat(), cluster_dir,
                "", "", 0,
                request.postgresql_version, request.allowed_ip_1, request.allowed_ip_2,
                public_ip
            ))
            conn.commit()
            conn.close()

            return {
                "message": "Kubernetes PostgreSQL deployed",
                "cluster_name": request.cluster_name,
                "platform": request.platform,
                "kubernetes_service": f"{request.cluster_name}.local.cluster",
                "port": 5432,
                "user": "postgres",
                "password": "postgres"
            }

        # Proceed with EC2 (bare metal) deployment
        tf_module_dst = os.path.join(cluster_dir, "terraform", "modules", "postgres_ha")
        tfvars_path = os.path.join(tf_module_dst, "terraform.tfvars")
        groupvars_path = os.path.join(ansible_dst, "group_vars", "all.yml")

        shutil.copytree(os.path.join(TEMPLATE_DIR, "ansible"), ansible_dst, dirs_exist_ok=True)
        os.makedirs(os.path.dirname(tf_module_dst), exist_ok=True)
        shutil.copytree(os.path.join(TEMPLATE_DIR, "terraform", "modules", "postgres_ha"), tf_module_dst)

        tf_template = env.get_template("terraform/terraform.tfvars.j2")
        ssh_user = OS_AMI_USER_MAPPING.get(request.ami, "rocky")
        with open(tfvars_path, "w") as f:
            f.write(tf_template.render(
                cluster_name=request.cluster_name,
                instance_count=request.instance_count,
                postgres_version=request.postgresql_version,
                ami=request.ami,
                instance_type=request.instance_type,
                data_volume_size=request.data_volume_size,
                ssh_user=ssh_user,
                key_name="ha-postgres-key",
                public_key_path="~/.ssh/ha-postgres-key.pub",
                allowed_ip_1=request.allowed_ip_1,
                allowed_ip_2=request.allowed_ip_2,
                server_public_ip=public_ip
            ))

        groupvars_template = env.get_template("ansible/group_vars/all.yml")
        with open(groupvars_path, "w") as f:
            f.write(groupvars_template.render(
                cluster_name=request.cluster_name,
                postgresql_version=request.postgresql_version,
                instance_count=request.instance_count,
                allowed_ips=[request.allowed_ip_1, request.allowed_ip_2],
                server_public_ip=public_ip
            ))

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO clusters (
                cluster_name, platform, instance_count, status, timestamp,
                deployment_dir, ami, instance_type, data_volume_size,
                postgresql_version, allowed_ip_1, allowed_ip_2, server_public_ip
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            request.cluster_name, request.platform, request.instance_count,
            "in_progress", datetime.utcnow().isoformat(), cluster_dir,
            request.ami, request.instance_type, request.data_volume_size,
            request.postgresql_version, request.allowed_ip_1, request.allowed_ip_2,
            public_ip
        ))
        conn.commit()
        conn.close()

                # Terraform provisioning
        subprocess.run(["terraform", "init"], cwd=tf_module_dst, check=True)
        subprocess.run(["terraform", "apply", "-auto-approve"], cwd=tf_module_dst, check=True)

        output_result = subprocess.run(
            ["terraform", "output", "-json"], cwd=tf_module_dst,
            capture_output=True, text=True, check=True
        )
        tf_outputs = json.loads(output_result.stdout)
        node_ips = tf_outputs.get("node_ips", {}).get("value", [])
        public_ips = tf_outputs.get("public_ips", {}).get("value", [])

        with open(groupvars_path) as f:
            raw_vars = f.read()
            rendered = env.from_string(raw_vars).render(
                cluster_name=request.cluster_name,
                postgresql_version=request.postgresql_version,
                instance_count=request.instance_count,
                allowed_ips=[request.allowed_ip_1, request.allowed_ip_2],
                server_public_ip=public_ip
            )
            vars_dict = yaml.safe_load(rendered)

        cluster_info = {
            "message": "Cluster provisioned successfully",
            "cluster_name": request.cluster_name,
            "platform": request.platform,
            "instance_count": request.instance_count,
            "external_access": {
                "public_ip": public_ip
            },
            "keepalived": {
                "auth_pass": vars_dict.get("keepalived_auth_pass"),
                "virtual_ip": vars_dict.get("keepalived_virtual_ip")
            },
            "haproxy": {
                "listen_port": vars_dict.get("haproxy_listen_port")
            },
            "postgresql": {
                "port": vars_dict.get("postgres_port"),
                "superuser_password": vars_dict.get("patroni_superuser_password"),
                "replication_user": vars_dict.get("patroni_replication_username"),
                "replication_password": vars_dict.get("patroni_replication_password")
            },
            "nodes": []
        }

        for i, (priv_ip, pub_ip) in enumerate(zip(node_ips, public_ips)):
            cluster_info["nodes"].append({
                "name": f"{request.cluster_name}-node-{i+1}",
                "private_ip": priv_ip,
                "public_ip": pub_ip,
                "patroni_api": f"http://{priv_ip}:8008"
            })

        if public_ips:
            cluster_info["monitoring"] = {
                "prometheus": f"http://{public_ips[-1]}:9090",
                "grafana": f"http://{public_ips[-1]}:3000"
            }

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("UPDATE clusters SET status=? WHERE cluster_name=?", ("completed", request.cluster_name))
        conn.commit()
        conn.close()

        return cluster_info

    except Exception as e:
        tf_exec_dir = os.path.join(cluster_dir, "terraform", "modules", "postgres_ha")
        try:
            if request.platform == "ec2" and os.path.exists(tf_exec_dir):
                subprocess.run(["terraform", "destroy", "-auto-approve"], cwd=tf_exec_dir, check=False)
        except Exception as tf_err:
            print(f"Terraform destroy failed during cleanup: {tf_err}")

        shutil.rmtree(cluster_dir, ignore_errors=True)
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM clusters WHERE cluster_name=?", (request.cluster_name,))
        conn.commit()
        conn.close()
        raise HTTPException(status_code=500, detail=f"Provisioning failed: {str(e)}")

#### Create Database ####
@app.post("/create_database")
def create_database(request: CreateDBRequest):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT deployment_dir, platform FROM clusters WHERE cluster_name=?", (request.cluster_name,))
    row = cursor.fetchone()
    conn.close()

    if not row:
        raise HTTPException(status_code=404, detail="Cluster not found")

    deployment_dir, platform = row
    if platform == "kubernetes":
        k8s_playbook = os.path.join(deployment_dir, "ansible", "create_database_k8s.yml")
        if not os.path.exists(k8s_playbook):
            raise HTTPException(status_code=500, detail="Kubernetes create DB playbook not found")
        try:
            subprocess.run([
                "ansible-playbook", k8s_playbook,
                "-e", f"db_name={request.db_name}"
            ], check=True)
            return {"message": f"[K8s] Database '{request.db_name}' created successfully"}
        except subprocess.CalledProcessError:
            raise HTTPException(status_code=500, detail="[K8s] Failed to create database")
    else:
        ansible_dir = os.path.join(deployment_dir, "ansible")
        inventory = os.path.join(ansible_dir, "inventory", "inventory.ini")
        playbook = os.path.join(ansible_dir, "create_database.yml")

        if not os.path.exists(playbook):
            raise HTTPException(status_code=500, detail="Ansible create_database.yml not found")

        try:
            subprocess.run([
                "ansible-playbook", "-i", inventory, playbook,
                "-e", f"db_name={request.db_name}"
            ], check=True)
            return {"message": f"Database '{request.db_name}' created successfully"}
        except subprocess.CalledProcessError:
            raise HTTPException(status_code=500, detail="Failed to create database")

@app.get("/clusters/{cluster_name}/databases")
def list_databases(cluster_name: str):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT deployment_dir, platform FROM clusters WHERE cluster_name=?", (cluster_name,))
    row = cursor.fetchone()
    conn.close()

    if not row:
        raise HTTPException(status_code=404, detail="Cluster not found")

    deployment_dir, platform = row
    if platform == "kubernetes":
        k8s_playbook = os.path.join(deployment_dir, "ansible", "list_databases_k8s.yml")
        if not os.path.exists(k8s_playbook):
            raise HTTPException(status_code=500, detail="list_databases_k8s.yml not found")
        try:
            result = subprocess.run(
                ["ansible-playbook", k8s_playbook],
                capture_output=True,
                text=True,
                check=True
            )
            match = re.search(r'"msg": (\[.*?\])', result.stdout, re.DOTALL)
            if match:
                dbs = ast.literal_eval(match.group(1))
                return {"databases": dbs}
            return {"databases": []}
        except subprocess.CalledProcessError as e:
            raise HTTPException(status_code=500, detail="[K8s] Failed to list databases: " + e.stderr.strip())
    else:
        ansible_dir = os.path.join(deployment_dir, "ansible")
        inventory_file = os.path.join(ansible_dir, "inventory", "inventory.ini")
        playbook_path = os.path.join(ansible_dir, "list_databases.yml")

        if not os.path.exists(playbook_path):
            raise HTTPException(status_code=500, detail="list_databases.yml not found")

        try:
            result = subprocess.run(
                ["ansible-playbook", "-i", inventory_file, playbook_path],
                capture_output=True,
                text=True,
                check=True
            )
            match = re.search(r'"msg": (\[.*?\])', result.stdout, re.DOTALL)
            if match:
                dbs = ast.literal_eval(match.group(1))
                return {"databases": dbs}
            return {"databases": []}
        except subprocess.CalledProcessError as e:
            raise HTTPException(status_code=500, detail="Failed to list databases: " + e.stderr.strip())

@app.post("/drop_database")
def drop_database(request: DropDBRequest):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT deployment_dir, platform FROM clusters WHERE cluster_name=?", (request.cluster_name,))
    row = cursor.fetchone()
    conn.close()

    if not row:
        raise HTTPException(status_code=404, detail="Cluster not found")

    deployment_dir, platform = row

    if platform == "kubernetes":
        k8s_playbook = os.path.join(deployment_dir, "ansible", "drop_database_k8s.yml")
        if not os.path.exists(k8s_playbook):
            raise HTTPException(status_code=500, detail="drop_database_k8s.yml not found")
        try:
            subprocess.run([
                "ansible-playbook",
                k8s_playbook,
                "-e", f"db_name={request.db_name}"
            ], check=True)
            return {"message": f"[K8s] Database '{request.db_name}' dropped successfully"}
        except subprocess.CalledProcessError:
            raise HTTPException(status_code=500, detail="[K8s] Failed to drop database")
    else:
        ansible_dir = os.path.join(deployment_dir, "ansible")
        inventory = os.path.join(ansible_dir, "inventory", "inventory.ini")
        playbook = os.path.join(ansible_dir, "drop_database.yml")

        if not os.path.exists(playbook):
            raise HTTPException(status_code=500, detail="Ansible drop_database.yml not found")

        try:
            subprocess.run([
                "ansible-playbook",
                "-i", inventory,
                playbook,
                "-e", f"db_name={request.db_name}"
            ], check=True)
            return {"message": f"Database '{request.db_name}' dropped successfully"}
        except subprocess.CalledProcessError:
            raise HTTPException(status_code=500, detail="Failed to drop database")

@app.post("/decommission")
def decommission_standalone(request: DecommissionRequest):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT instance_count, deployment_dir, platform FROM clusters WHERE cluster_name=?", (request.cluster_name,))
    row = cursor.fetchone()
    conn.close()

    if not row:
        raise HTTPException(status_code=404, detail="Cluster not found")

    instance_count, deployment_dir, platform = row

    if platform == "kubernetes":
        try:
            k8s_cleanup = os.path.join(deployment_dir, "ansible", "k8s_cleanup.yml")
            if os.path.exists(k8s_cleanup):
                subprocess.run(["ansible-playbook", k8s_cleanup], check=True)

            shutil.rmtree(deployment_dir, ignore_errors=True)

            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("DELETE FROM clusters WHERE cluster_name=?", (request.cluster_name,))
            conn.commit()
            conn.close()

            return {"message": f"Kubernetes cluster '{request.cluster_name}' decommissioned successfully"}
        except subprocess.CalledProcessError as e:
            raise HTTPException(status_code=500, detail=f"K8s decommission failed: {str(e)}")
    else:
        if instance_count > 1:
            raise HTTPException(status_code=400, detail="Decommission is only supported for standalone EC2 clusters")

        tf_exec_dir = os.path.join(deployment_dir, "terraform", "modules", "postgres_ha")
        if not os.path.exists(tf_exec_dir):
            raise HTTPException(status_code=500, detail="Terraform path not found")

        try:
            subprocess.run(["terraform", "destroy", "-auto-approve"], cwd=tf_exec_dir, check=True)
            shutil.rmtree(deployment_dir, ignore_errors=True)

            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("DELETE FROM clusters WHERE cluster_name=?", (request.cluster_name,))
            conn.commit()
            conn.close()

            return {"message": f"Cluster '{request.cluster_name}' decommissioned successfully"}
        except subprocess.CalledProcessError as e:
            raise HTTPException(status_code=500, detail="Terraform destroy failed")

@app.post("/add_user")
def add_user(request: AddUserRequest):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT deployment_dir, platform FROM clusters WHERE cluster_name=?", (request.cluster_name,))
    row = cursor.fetchone()
    conn.close()

    if not row:
        raise HTTPException(status_code=404, detail="Cluster not found")

    deployment_dir, platform = row
    ansible_dir = os.path.join(deployment_dir, "ansible")

    if platform == "kubernetes":
        playbook = os.path.join(ansible_dir, "k8s_add_user.yml")
        inventory = None  # Not needed for local playbook execution
    else:
        inventory = os.path.join(ansible_dir, "inventory", "inventory.ini")
        playbook = os.path.join(ansible_dir, "add_user.yml")

    if not os.path.exists(playbook):
        raise HTTPException(status_code=500, detail="Add user playbook not found")

    cmd = ["ansible-playbook"]
    if inventory:
        cmd += ["-i", inventory]
    cmd += [
        playbook,
        "-e", f"db_user={request.username}",
        "-e", f"db_pass={request.password}",
        "-e", f"db_roles={','.join(request.roles)}",
        "-e", f"db_name={request.database}"
    ]

    try:
        subprocess.run(cmd, check=True)
        return {"message": f"User '{request.username}' added successfully"}
    except subprocess.CalledProcessError:
        raise HTTPException(status_code=500, detail="Failed to add user")

@app.post("/remove_user")
def remove_user(request: RemoveUserRequest):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT deployment_dir, platform FROM clusters WHERE cluster_name=?", (request.cluster_name,))
    row = cursor.fetchone()
    conn.close()

    if not row:
        raise HTTPException(status_code=404, detail="Cluster not found")

    deployment_dir, platform = row
    ansible_dir = os.path.join(deployment_dir, "ansible")

    if platform == "kubernetes":
        playbook = os.path.join(ansible_dir, "k8s_remove_user.yml")
        inventory = None
    else:
        inventory = os.path.join(ansible_dir, "inventory", "inventory.ini")
        playbook = os.path.join(ansible_dir, "remove_user.yml")

    if not os.path.exists(playbook):
        raise HTTPException(status_code=500, detail="Remove user playbook not found")

    cmd = ["ansible-playbook"]
    if inventory:
        cmd += ["-i", inventory]
    cmd += [
        playbook,
        "-e", f"db_user={request.username}"
    ]

    try:
        subprocess.run(cmd, check=True)
        return {"message": f"User '{request.username}' removed successfully"}
    except subprocess.CalledProcessError:
        raise HTTPException(status_code=500, detail="Failed to remove user")


@app.post("/scale")
def scale_cluster():
    return placeholder_response("scale")






@app.post("/deploy")
def deploy_cluster(request: DeployRequest):
    if request.instance_count > 5:
        raise HTTPException(status_code=400, detail="Maximum 5 instances allowed.")

    cluster_dir = os.path.join(DEPLOYMENTS_DIR, request.cluster_name)
    if os.path.exists(cluster_dir):
        raise HTTPException(status_code=409, detail="Cluster already exists.")
    os.makedirs(cluster_dir, exist_ok=True)

    try:
        server_public_ip = requests.get("https://api.ipify.org").text.strip()
        ansible_dst = os.path.join(cluster_dir, "ansible")
        os.makedirs(ansible_dst, exist_ok=True)
        shutil.copytree(os.path.join(TEMPLATE_DIR, "ansible"), ansible_dst, dirs_exist_ok=True)

        groupvars_path = os.path.join(ansible_dst, "group_vars", "all.yml")
        groupvars_template = env.get_template("ansible/group_vars/all.yml")
        with open(groupvars_path, "w") as f:
            f.write(groupvars_template.render(
                cluster_name=request.cluster_name,
                postgresql_version=request.postgresql_version,
                instance_count=request.instance_count,
                allowed_ips=[request.allowed_ip_1, request.allowed_ip_2],
                server_public_ip=server_public_ip,
                platform=request.platform,
                pod_count=request.pod_count if request.platform == "kubernetes" else None
            ))

        tf_module_dst = os.path.join(cluster_dir, "terraform", "modules", "postgres_ha")
        tfvars_path = os.path.join(tf_module_dst, "terraform.tfvars")
        os.makedirs(os.path.dirname(tf_module_dst), exist_ok=True)
        shutil.copytree(os.path.join(TEMPLATE_DIR, "terraform", "modules", "postgres_ha"), tf_module_dst, dirs_exist_ok=True)

        tf_template = env.get_template("terraform/terraform.tfvars.j2")
        ssh_user = OS_AMI_USER_MAPPING.get(request.ami, "rocky")
        with open(tfvars_path, "w") as f:
            f.write(tf_template.render(
                cluster_name=request.cluster_name,
                instance_count=request.instance_count,
                postgres_version=request.postgresql_version,
                ami=request.ami,
                instance_type=request.instance_type,
                data_volume_size=request.data_volume_size,
                ssh_user=ssh_user,
                key_name="ha-postgres-key",
                public_key_path="~/.ssh/ha-postgres-key.pub",
                allowed_ip_1=request.allowed_ip_1,
                allowed_ip_2=request.allowed_ip_2,
                server_public_ip=server_public_ip,
                platform=request.platform,
                pod_count=request.pod_count if request.platform == "kubernetes" else None
            ))

        subprocess.run(["terraform", "init"], cwd=tf_module_dst, check=True)
        subprocess.run(["terraform", "apply", "-auto-approve"], cwd=tf_module_dst, check=True)

        output_result = subprocess.run(
            ["terraform", "output", "-json"], cwd=tf_module_dst,
            capture_output=True, text=True, check=True
        )
        tf_outputs = json.loads(output_result.stdout)
        node_ips = tf_outputs.get("node_ips", {}).get("value", [])
        public_ips = tf_outputs.get("public_ips", {}).get("value", [])

        pub_ip_fields = public_ips + [None] * (9 - len(public_ips))
        priv_ip_fields = node_ips + [None] * (9 - len(node_ips))

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO clusters (
                cluster_name, platform, instance_count, status, timestamp,
                deployment_dir, ami, instance_type, data_volume_size,
                postgresql_version, allowed_ip_1, allowed_ip_2, server_public_ip,
                public_ip_1, public_ip_2, public_ip_3, public_ip_4, public_ip_5,
                public_ip_6, public_ip_7, public_ip_8, public_ip_9,
                private_ip_1, private_ip_2, private_ip_3, private_ip_4, private_ip_5,
                private_ip_6, private_ip_7, private_ip_8, private_ip_9
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                      ?, ?, ?, ?, ?, ?, ?, ?, ?,
                      ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            request.cluster_name, request.platform, request.instance_count,
            "in_progress", datetime.utcnow().isoformat(), cluster_dir,
            request.ami, request.instance_type, request.data_volume_size,
            request.postgresql_version, request.allowed_ip_1, request.allowed_ip_2,
            server_public_ip,
            *pub_ip_fields, *priv_ip_fields
        ))
        conn.commit()
        conn.close()

        with open(groupvars_path) as f:
            raw_vars = f.read()
            rendered = env.from_string(raw_vars).render(
                cluster_name=request.cluster_name,
                postgresql_version=request.postgresql_version,
                instance_count=request.instance_count,
                allowed_ips=[request.allowed_ip_1, request.allowed_ip_2],
                server_public_ip=server_public_ip,
                platform=request.platform
            )
            vars_dict = yaml.safe_load(rendered)

        cluster_info = {
            "message": "Cluster provisioned successfully",
            "cluster_name": request.cluster_name,
            "platform": request.platform,
            "instance_count": request.instance_count,
            "external_access": {
                "server_public_ip": server_public_ip
            },
            "keepalived": {
                "auth_pass": vars_dict.get("keepalived_auth_pass"),
                "virtual_ip": vars_dict.get("keepalived_virtual_ip")
            },
            "haproxy": {
                "listen_port": vars_dict.get("haproxy_listen_port")
            },
            "postgresql": {
                "port": vars_dict.get("postgres_port"),
                "superuser_password": vars_dict.get("patroni_superuser_password"),
                "replication_user": vars_dict.get("patroni_replication_username"),
                "replication_password": vars_dict.get("patroni_replication_password")
            },
            "nodes": []
        }

        for i, (priv_ip, pub_ip) in enumerate(zip(node_ips, public_ips)):
            cluster_info["nodes"].append({
                "name": f"{request.cluster_name}-node-{i+1}",
                "private_ip": priv_ip,
                "public_ip": pub_ip,
                "patroni_api": f"http://{priv_ip}:8008"
            })

        if public_ips:
            cluster_info["monitoring"] = {
                "prometheus": f"http://{public_ips[-1]}:9090",
                "grafana": f"http://{public_ips[-1]}:3000"
            }

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("UPDATE clusters SET status=? WHERE cluster_name=?", ("completed", request.cluster_name))
        conn.commit()
        conn.close()

        return cluster_info

    except Exception as e:
        tf_exec_dir = os.path.join(cluster_dir, "terraform", "modules", "postgres_ha")
        try:
            if request.platform == "ec2" and os.path.exists(tf_exec_dir):
                subprocess.run(["terraform", "destroy", "-auto-approve"], cwd=tf_exec_dir, check=False)
        except Exception as tf_err:
            print(f"Terraform destroy failed during cleanup: {tf_err}")

        shutil.rmtree(cluster_dir, ignore_errors=True)
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM clusters WHERE cluster_name=?", (request.cluster_name,))
        conn.commit()
        conn.close()
        raise HTTPException(status_code=500, detail=f"Provisioning failed: {str(e)}")
 